<?php
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: google/cloud/ml/v1beta1/job_service.proto

namespace Google\Cloud\Ml\V1beta1;

use Google\Protobuf\Internal\GPBType;
use Google\Protobuf\Internal\RepeatedField;
use Google\Protobuf\Internal\GPBUtil;

/**
 * <pre>
 * Represents a set of hyperparameters to optimize.
 * </pre>
 *
 * Protobuf type <code>google.cloud.ml.v1beta1.HyperparameterSpec</code>
 */
class HyperparameterSpec extends \Google\Protobuf\Internal\Message
{
    /**
     * <pre>
     * Required. The type of goal to use for tuning. Available types are
     * `MAXIMIZE` and `MINIMIZE`.
     * Defaults to `MAXIMIZE`.
     * </pre>
     *
     * <code>.google.cloud.ml.v1beta1.HyperparameterSpec.GoalType goal = 1;</code>
     */
    private $goal = 0;
    /**
     * <pre>
     * Required. The set of parameters to tune.
     * </pre>
     *
     * <code>repeated .google.cloud.ml.v1beta1.ParameterSpec params = 2;</code>
     */
    private $params;
    /**
     * <pre>
     * Optional. How many training trials should be attempted to optimize
     * the specified hyperparameters.
     * Defaults to one.
     * </pre>
     *
     * <code>int32 max_trials = 3;</code>
     */
    private $max_trials = 0;
    /**
     * <pre>
     * Optional. The number of training trials to run concurrently.
     * You can reduce the time it takes to perform hyperparameter tuning by adding
     * trials in parallel. However, each trail only benefits from the information
     * gained in completed trials. That means that a trial does not get access to
     * the results of trials running at the same time, which could reduce the
     * quality of the overall optimization.
     * Each trial will use the same scale tier and machine types.
     * Defaults to one.
     * </pre>
     *
     * <code>int32 max_parallel_trials = 4;</code>
     */
    private $max_parallel_trials = 0;
    /**
     * <pre>
     * Optional. The Tensorflow summary tag name to use for optimizing trials. For
     * current versions of Tensorflow, this tag name should exactly match what is
     * shown in Tensorboard, including all scopes.  For versions of Tensorflow
     * prior to 0.12, this should be only the tag passed to tf.Summary.
     * By default, "training/hptuning/metric" will be used.
     * </pre>
     *
     * <code>string hyperparameter_metric_tag = 5;</code>
     */
    private $hyperparameter_metric_tag = '';

    public function __construct() {
        \GPBMetadata\Google\Cloud\Ml\V1Beta1\JobService::initOnce();
        parent::__construct();
    }

    /**
     * <pre>
     * Required. The type of goal to use for tuning. Available types are
     * `MAXIMIZE` and `MINIMIZE`.
     * Defaults to `MAXIMIZE`.
     * </pre>
     *
     * <code>.google.cloud.ml.v1beta1.HyperparameterSpec.GoalType goal = 1;</code>
     */
    public function getGoal()
    {
        return $this->goal;
    }

    /**
     * <pre>
     * Required. The type of goal to use for tuning. Available types are
     * `MAXIMIZE` and `MINIMIZE`.
     * Defaults to `MAXIMIZE`.
     * </pre>
     *
     * <code>.google.cloud.ml.v1beta1.HyperparameterSpec.GoalType goal = 1;</code>
     */
    public function setGoal($var)
    {
        GPBUtil::checkEnum($var, \Google\Cloud\Ml\V1beta1\HyperparameterSpec_GoalType::class);
        $this->goal = $var;
    }

    /**
     * <pre>
     * Required. The set of parameters to tune.
     * </pre>
     *
     * <code>repeated .google.cloud.ml.v1beta1.ParameterSpec params = 2;</code>
     */
    public function getParams()
    {
        return $this->params;
    }

    /**
     * <pre>
     * Required. The set of parameters to tune.
     * </pre>
     *
     * <code>repeated .google.cloud.ml.v1beta1.ParameterSpec params = 2;</code>
     */
    public function setParams(&$var)
    {
        GPBUtil::checkRepeatedField($var, \Google\Protobuf\Internal\GPBType::MESSAGE, \Google\Cloud\Ml\V1beta1\ParameterSpec::class);
        $this->params = $var;
    }

    /**
     * <pre>
     * Optional. How many training trials should be attempted to optimize
     * the specified hyperparameters.
     * Defaults to one.
     * </pre>
     *
     * <code>int32 max_trials = 3;</code>
     */
    public function getMaxTrials()
    {
        return $this->max_trials;
    }

    /**
     * <pre>
     * Optional. How many training trials should be attempted to optimize
     * the specified hyperparameters.
     * Defaults to one.
     * </pre>
     *
     * <code>int32 max_trials = 3;</code>
     */
    public function setMaxTrials($var)
    {
        GPBUtil::checkInt32($var);
        $this->max_trials = $var;
    }

    /**
     * <pre>
     * Optional. The number of training trials to run concurrently.
     * You can reduce the time it takes to perform hyperparameter tuning by adding
     * trials in parallel. However, each trail only benefits from the information
     * gained in completed trials. That means that a trial does not get access to
     * the results of trials running at the same time, which could reduce the
     * quality of the overall optimization.
     * Each trial will use the same scale tier and machine types.
     * Defaults to one.
     * </pre>
     *
     * <code>int32 max_parallel_trials = 4;</code>
     */
    public function getMaxParallelTrials()
    {
        return $this->max_parallel_trials;
    }

    /**
     * <pre>
     * Optional. The number of training trials to run concurrently.
     * You can reduce the time it takes to perform hyperparameter tuning by adding
     * trials in parallel. However, each trail only benefits from the information
     * gained in completed trials. That means that a trial does not get access to
     * the results of trials running at the same time, which could reduce the
     * quality of the overall optimization.
     * Each trial will use the same scale tier and machine types.
     * Defaults to one.
     * </pre>
     *
     * <code>int32 max_parallel_trials = 4;</code>
     */
    public function setMaxParallelTrials($var)
    {
        GPBUtil::checkInt32($var);
        $this->max_parallel_trials = $var;
    }

    /**
     * <pre>
     * Optional. The Tensorflow summary tag name to use for optimizing trials. For
     * current versions of Tensorflow, this tag name should exactly match what is
     * shown in Tensorboard, including all scopes.  For versions of Tensorflow
     * prior to 0.12, this should be only the tag passed to tf.Summary.
     * By default, "training/hptuning/metric" will be used.
     * </pre>
     *
     * <code>string hyperparameter_metric_tag = 5;</code>
     */
    public function getHyperparameterMetricTag()
    {
        return $this->hyperparameter_metric_tag;
    }

    /**
     * <pre>
     * Optional. The Tensorflow summary tag name to use for optimizing trials. For
     * current versions of Tensorflow, this tag name should exactly match what is
     * shown in Tensorboard, including all scopes.  For versions of Tensorflow
     * prior to 0.12, this should be only the tag passed to tf.Summary.
     * By default, "training/hptuning/metric" will be used.
     * </pre>
     *
     * <code>string hyperparameter_metric_tag = 5;</code>
     */
    public function setHyperparameterMetricTag($var)
    {
        GPBUtil::checkString($var, True);
        $this->hyperparameter_metric_tag = $var;
    }

}

